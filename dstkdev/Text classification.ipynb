{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cytoolz.curried import get, groupby, valmap\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from nltk.stem.snowball import GermanStemmer\n",
    "from collections import ChainMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [\"abc def\\nabc def\",\n",
    "        \"def\\nghi\",\n",
    "       ]*20\n",
    "\n",
    "labels = [0, 1]*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NoFit:\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    \n",
    "class Doc2List(BaseEstimator, TransformerMixin, NoFit):\n",
    "    def transform(self, docs):\n",
    "        return [[line.split() for line in doc.split(\"\\n\")] for doc in docs]\n",
    "\n",
    "    \n",
    "class List2doc(BaseEstimator, TransformerMixin, NoFit):   \n",
    "    def transform(self, doclists):\n",
    "        return [\"\\n\".join(\" \".join(line) for line in doclist) for doclist in doclists] \n",
    "    \n",
    "    \n",
    "class DocTrans(BaseEstimator, TransformerMixin, NoFit):\n",
    "    def __init__(self, doc_func):\n",
    "        self.doc_func = doc_func\n",
    "        \n",
    "    def transform(self, docs):\n",
    "        return [self.doc_func(doc) for doc in docs]\n",
    "   \n",
    "\n",
    "class DocFunc(BaseEstimator, TransformerMixin, NoFit):\n",
    "    def __init__(self, doc_func):\n",
    "        self.doc_func = doc_func\n",
    "        \n",
    "    def transform(self, docs):\n",
    "        return [[self.doc_func(doc)] for doc in docs]\n",
    "  \n",
    "\n",
    "class CleanDoc(BaseEstimator, TransformerMixin, NoFit):\n",
    "    def __init__(self):\n",
    "        self.stemmer = GermanStemmer()\n",
    "        \n",
    "    def transform(self, docs):\n",
    "        res = []\n",
    "        for doc in docs:\n",
    "            lines = doc.split(\"\\n\")\n",
    "            lines = [\" \".join(self.stemmer.stem(word)\n",
    "                              for word in re.findall(\"[a-zäöüß]{3,}\", line.lower()))\n",
    "                     for line in lines]\n",
    "            res.append(\"\\n\".join(lines))\n",
    "            \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subwords(word):\n",
    "    return [word[:2], word[2:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = GermanStemmer().stem\n",
    "\n",
    "cnt_vect_splits = [(\"short\", lambda doc: [line for line in doc if len(line)<=1], {}),\n",
    "                   (\"long\", lambda doc: [line for line in doc if len(line)>1], {}),\n",
    "                   (\"subwords\", lambda doc: [list(map(stem, concat(subwords(word) for word in line))) for line in doc], {\"ngram_range\":(1,1)}),\n",
    "                  ]\n",
    "\n",
    "doc_funcs = [(\"num_char\", lambda doc: len(re.findall(\"[A-Za-zäöüÄÖÜß]\", doc))),\n",
    "            ]\n",
    "\n",
    "cnt_vect_default_params = {\"min_df\": 3,\n",
    "                           \"strip_accents\": \"unicode\",\n",
    "                           \"ngram_range\": (1,2),\n",
    "                           #\"stop_words\": [...],\n",
    "                          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pipe = Pipeline([(\"doc2list\", Doc2List()),\n",
    "                       (\"cnt_vects\", FeatureUnion([(\"cnt_vect_pipe_{}\".format(filter_name),\n",
    "                                                    Pipeline([(\"filter_{}\".format(filter_name), DocTrans(filter_func)),\n",
    "                                                              (\"list2doc\", List2doc()),\n",
    "                                                              (\"cnt_vect_{}\".format(filter_name), CountVectorizer(**ChainMap(filter_cnt_vect_param,\n",
    "                                                                                                                             cnt_vect_default_params))),\n",
    "                                                             ])\n",
    "                                                   ) for filter_name, filter_func, filter_cnt_vect_param in cnt_vect_splits\n",
    "                                                  ])),\n",
    "                       (\"tfidf\", TfidfTransformer(sublinear_tf=True)),\n",
    "                      ])\n",
    "\n",
    "clf = Pipeline([(\"clean\", CleanDoc()),\n",
    "                (\"tfidf_and_meta\", FeatureUnion([(\"tfidf_pipe\", tfidf_pipe)]+ # needs to be first, or below vocabulary_map miscounts\n",
    "                                                [(name, DocFunc(func)) for name, func in doc_funcs])),\n",
    "                (\"clf\", LogisticRegressionCV()),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(docs, labels)\n",
    "clf.predict(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_step_by_name(pipe, name):\n",
    "    return [trans for name_, trans in pipe.steps if name_.startswith(name)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ 9⌀\n",
       "  ab : [ 7 ]\n",
       "  abc : [ 3 ]\n",
       "  abc def : [ 5 ]\n",
       "  de : [ 8 ]\n",
       "  def : [ 0,  4 ]\n",
       "  def abc : [ 6 ]\n",
       "  def ghi : [ 2 ]\n",
       "  gh : [ 9 ]\n",
       "  ghi : [ 1 ] }"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_vects_pipe = get_step_by_name(tfidf_pipe, \"cnt_vects\")\n",
    "\n",
    "cnt_vects = [get_step_by_name(pipe, \"cnt_vect_\") for _name, pipe in cnt_vects_pipe.transformer_list]\n",
    "\n",
    "vocabulary_map = pipe(enumerate(concat(cnt_vect.vocabulary_ for cnt_vect in cnt_vects)),\n",
    "                      groupby(get(1)),\n",
    "                      valmap(lambda vals:list(pluck(0, vals))),\n",
    "                     )\n",
    "vocabulary_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
